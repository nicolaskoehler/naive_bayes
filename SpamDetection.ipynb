{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57c8f53-e9fa-42c9-9813-96b7532278fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading input data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"data/data_email.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc85f361-169b-47b3-a595-68fb456f66a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing input data_X into separate words and labels into [0,1]; defining X and c\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(df['data_X']).toarray()\n",
    "lb = LabelBinarizer()\n",
    "c = lb.fit_transform(df['data_c']).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e9809a-161a-4df6-aed3-35b1b0a610aa",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Naive Bayes classifier: $\\arg \\max_{1\\leq i\\leq n} \\left\\{ \\log \\left( \\prod_{k=1}^m P(x^{(k)}|c_i) \\cdot P(c_i) \\right) \\right\\} = \\arg \\max_{1\\leq i\\leq n} \\left\\{ \\sum_{k=1}^m \\log \\left(  P(x^{(k)}|c_i) \\right) + \\log \\left( P(c_i) \\right) \\right\\}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b91a334-97d4-49e7-81d7-23d0c7fd75be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement Naive Bayes for discrete variables\n",
    "class NaiveBayes:\n",
    "    # training \n",
    "    def fit(self, X, c):\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        self._classes, self._classCounts = np.unique(c,return_counts=True)\n",
    "        self.n_classes = len(self._classes)\n",
    "        self.alpha = 1.\n",
    "        self._priors = np.zeros(self.n_classes)\n",
    "        self._P_words_feature = np.zeros((self.n_classes, self.n_features))\n",
    "        for idx, c_i in enumerate(self._classes):\n",
    "            self._priors[idx] = np.log(self._classCounts[idx]/self.n_samples)\n",
    "            X_c_i = X[c==c_i]\n",
    "            column_sum = np.zeros(self.n_features)\n",
    "            for iFeature in range(self.n_features):\n",
    "                column_sum[iFeature] = np.sum(X_c_i[:,iFeature])\n",
    "            self._P_words_feature[idx] = np.log((column_sum+self.alpha)/(np.sum(column_sum)+self.alpha*self.n_features))\n",
    "    # application\n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros(X.shape[0])\n",
    "        for idx, x in enumerate(X):\n",
    "            posteriors = np.zeros(self.n_classes)\n",
    "            for idx_c, c_i in enumerate(self._classes):\n",
    "                prior = self._priors[idx_c]\n",
    "                toSum = np.zeros(self.n_features)\n",
    "                for iFeature in range(self.n_features):\n",
    "                    toSum[iFeature] = x[iFeature]*self._P_words_feature[idx_c][iFeature]\n",
    "                posteriors[idx_c] = np.sum(toSum)+prior\n",
    "            predictions[idx]= self._classes[np.argmax(posteriors)]\n",
    "        return predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166bb707-26f4-4dbb-bb49-31731c74a066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split input data in training and testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, c_train, c_test = train_test_split(X,c, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cab267-9b0a-4398-a572-7f5034a7c5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and testing Naive Bayes\n",
    "nb = NaiveBayes()\n",
    "nb.fit(X_train,c_train)\n",
    "predictions = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b3d2d3-0aaa-47d0-80ca-fe90aa062cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing the same in scikit learn\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train,c_train)\n",
    "predictions_sk = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb276986-b962-47f3-bd2c-b6e18825395f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing accuracy\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.sum(y_true==y_pred)/len(y_true)\n",
    "\n",
    "print(\"accuracy = \", accuracy(c_test, predictions))\n",
    "print(\"accuracy (scikit learn) = \", accuracy(c_test, predictions_sk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6399774-40ae-4f9c-a54d-77ea8a69617f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print('confusion matrix:')\n",
    "print(confusion_matrix(c_test, predictions))\n",
    "print('confusion matrix (scikit learn):')\n",
    "print(confusion_matrix(c_test, predictions_sk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1725793f-f329-405f-845c-a83031f1cd07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "sklearn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
